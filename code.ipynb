{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download the needed libraries and restart the session"
      ],
      "metadata": {
        "id": "6vIFauS_HtVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!apt install ffmpeg\n",
        "!pip install pydub\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "eGvqSdGSDNUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVesqbm0FcM"
      },
      "source": [
        "# Image to text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the model from hugging face using transformers pipeline"
      ],
      "metadata": {
        "id": "VKIEudt8H1iA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZr2acG10FcR"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHWBAfvf0FcU"
      },
      "outputs": [],
      "source": [
        "image2text=pipe(\"profile.jpeg\")[0][\"generated_text\"]\n",
        "image2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K44FDbW0FcW"
      },
      "source": [
        "# Story Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check if the cuda working on ur computer"
      ],
      "metadata": {
        "id": "FlJ9OyBaH_ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GU2tdj6BOTB",
        "outputId": "ebec46d1-1892-4c71-9967-878fd56f68a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Q3z2XOzfHeBR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first start with the promt you want to use for the generation model"
      ],
      "metadata": {
        "id": "tXmYFpAYIFZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFSTe44L0FcX"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an expert storyteller.\n",
        "Your task is to generate a short story (20 to 30 words) based on a given sentence.\n",
        "Ensure the story has a clear beginning, middle, and end while maintaining engagement and coherence.\n",
        "Do not exceed 30 words.\n",
        "\"\"\"\n",
        "\n",
        "def format_dataset(examples):\n",
        "    formatted_prompts = []\n",
        "    formatted_stories = []\n",
        "    for i in range(len(examples[\"prompt\"])):  # Iterate over the examples\n",
        "        prompt = f\"{SYSTEM_PROMPT}\\n\\n**User:** {examples['prompt'][i]}\"\n",
        "        story = examples[\"story\"][i]\n",
        "        formatted_prompts.append(prompt)\n",
        "        formatted_stories.append(story)\n",
        "    return {\"prompt\": formatted_prompts, \"story\": formatted_stories}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset from the json file (story_generation_dataset)"
      ],
      "metadata": {
        "id": "56DM8fP4IMmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZkJqJ4J0FcX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('json', data_files='story_generation_dataset.json')\n",
        "# Apply the formatting function\n",
        "formatted_dataset = dataset.map(format_dataset, batched=True)\n",
        "\n",
        "# Select the first 10 samples\n",
        "subset_dataset = formatted_dataset['train'].select(range(10))\n",
        "\n",
        "# Inspect the subset\n",
        "print(subset_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset"
      ],
      "metadata": {
        "id": "6ibfvQy7ULEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load the model and the Tokenizer"
      ],
      "metadata": {
        "id": "DHvvzyLLIYJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsi35y9v0FcY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "BASE_MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(examples['prompt'], examples['story'], truncation=True, padding='max_length', max_length=256)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"]  # Add labels for causal LM\n",
        "    return tokenized\n",
        "\n",
        "tokenized_subset = subset_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Inspect the tokenized subset\n",
        "print(tokenized_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning the model using LoRA (Low-Rank Adaptation)"
      ],
      "metadata": {
        "id": "GAN1RWAUI0Ea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_3qSS6_0Fca"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Configure 8-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Enable 8-bit quantization\n",
        "    llm_int8_threshold=6.0,  # Threshold for quantization\n",
        ")\n",
        "\n",
        "# Load the model with 8-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",  # Automatically map layers to GPU/CPU\n",
        ")\n",
        "\n",
        "# Prepare the model for 8-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank adaptation\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Target modules for LoRA\n",
        "    lora_dropout=0.1,  # Dropout for LoRA\n",
        "    bias=\"none\",  # No bias\n",
        "    task_type=\"CAUSAL_LM\",  # Task type\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Small batch size to save memory\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=10_000,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_subset,\n",
        "    eval_dataset=tokenized_subset,  # Use the same subset for evaluation\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a story from a prompt"
      ],
      "metadata": {
        "id": "u8-7KAMlJS4E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mw_jxIA0Fcb"
      },
      "outputs": [],
      "source": [
        "input_prompt = image2text\n",
        "input_text = f\"{SYSTEM_PROMPT}\\n\\n**User:** {input_prompt}\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(model.device)\n",
        "\n",
        "# Generate the story\n",
        "output = model.generate(input_ids, max_length=200, num_return_sequences=1)\n",
        "generated_story = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_story)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_story"
      ],
      "metadata": {
        "id": "z5AMVqbKISRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract the story from the model reply"
      ],
      "metadata": {
        "id": "lomIe8rBJVOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_story(text):\n",
        "    # Remove the introductory text and unwanted newlines from the beginning\n",
        "    text = re.sub(r\"You are an expert storyteller.*?(\\n\\n|$)\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Case 1: Match format starting with '**Story:'\n",
        "    story_match = re.search(r\"\\*\\*Story:\\*\\*\\s*(.*)\", text, re.DOTALL)\n",
        "    if story_match:\n",
        "        return story_match.group(1).strip()\n",
        "\n",
        "    # Case 2: Match format starting after '---'\n",
        "    story_match = re.search(r\"---\\s*(.*)\", text, re.DOTALL)\n",
        "    if story_match:\n",
        "        return story_match.group(1).strip()\n",
        "\n",
        "    # Case 3: Match format starting with 'Beginner's Guide:'\n",
        "    story_match = re.search(r\"Beginner's Guide:.*\\*\\*(.*)\", text, re.DOTALL)\n",
        "    if story_match:\n",
        "        return story_match.group(1).strip()\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "k6prSRItE2Pz"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story=extract_story(generated_story)"
      ],
      "metadata": {
        "id": "yJTrDFGfJNjc"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story"
      ],
      "metadata": {
        "id": "NTL4OYH5LeVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ukDKwb0Fcd"
      },
      "source": [
        "# Text To Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the text2speech model from hugging face"
      ],
      "metadata": {
        "id": "QtWY5OnIJbNb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCZSKlX80Fcd"
      },
      "outputs": [],
      "source": [
        "pipe_text2speech = pipeline(\"text-to-speech\", model=\"facebook/mms-tts-eng\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying the model and saving the audio generated"
      ],
      "metadata": {
        "id": "7RfTarcKJlL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TkYnLh40Fcf"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "output=pipe_text2speech(story)\n",
        "audio_array = (output[\"audio\"] * 32767).astype(np.int16)  # Convert to 16-bit PCM format\n",
        "sample_rate = output[\"sampling_rate\"]\n",
        "\n",
        "audio_segment = AudioSegment(audio_array.tobytes(), frame_rate=sample_rate, sample_width=2, channels=1)\n",
        "\n",
        "# Save as MP3\n",
        "audio_segment.export(\"output.mp3\", format=\"mp3\")\n",
        "\n",
        "print(\"Audio saved as output.mp3\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}